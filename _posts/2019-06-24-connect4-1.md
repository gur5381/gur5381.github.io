---
title: (육목 강화학습 도전기) 사목 강화학습 따라하기
layout: post
date: '2019-06-24 03:00:00'
author: 줌코딩
tags: 육목학습
cover: "/assets/instacode.png"
categories: 육목학습
---

## 강화학습 도전기
나도 안다. 이것은 엄청 무모한 도전이다. 그냥 내가 완벽히 어떤 것을 배우고 싶은 의지나 강화학습이 재미있어 보여서 하는 것은 아니다.

그냥 지금까지 육목 알고리즘을 짜면서 아 강화를 진짜 꼭 한번 해보고 싶다는 마음이 있었다. 그것을 묵히고묵혀놓다가 드디어 해보게 되었다. 사실 강화하는 게 정말 강한 알고리즘을 만들어 내는 방법이 될 수 있을지는 잘 모르겠지만 조금은 기대한다. 강화가 아예 안될 수도 있지만 일단 도전해보자.

육목을 직접 학습하는 것은 조금 어려움이 있어서 기존에 된다고 올라와 있는 사목 강화학습 모델을 기반으로 도전해보려고 해보려고 한다. 일단 블로그 글을 쭉 따라가보겠다.

## 알파고 제로가 탄생하기 까지

2016년에 Deepmind에서 **알파고**를 탄생시켰다. 이 알파고는 당시 세계 챔피언이었던 이세돌을 4-1로 이기며 세계에 충격을 주었다. 이것 자체도 어마무지한 발전이었고 불가능으로 여겨졌던 일이었다.

하지만 2017년 10월에 Deepmind는 한걸음 더 나아가게 된다. "Mastering the Game of Go without Human Knowledge" 라는 논문을 통해 **알파고 제로**라는 새로운 형태의 알고리즘을 발표하게 된다. 알파고 제로는 알파고를 100-0으로 이기는 압도적인 알고리즘이다. 

딥마인드는 여기서 그치지 않고 약 48일 후에 알파고 제로가 어떻게 다른 게임에도 적용시킬 수 있는지 그 가능성을 담은 논문을 발표하게 된다. 이것과 함께 탄생한 것이 **알파제로**이다.

알파제로의 어마무시한 특징은 다음과 같다.

### 1. 알파제로는 인간의 인풋이 전혀 필요하지 않다.
이 말은 알파고 제로는 어느 게임에나 적용될 수 있다는 것을 의미한다. 

### 2. 알고리즘이 엘레강스하다.
알고리즘이 복잡해도 대박이라고 이야기하겠지만 심지어 알파제로에 있는 아이디어는 생각보다 훨 단순하다. 

이 알고리즘의 원리는 다음과 같다. 우리가 어떤 것을 배울 때 하는 과정과 얼마나 비슷한지 한번 보아라.

1. 괜찮은 길에게 우선순위를 주며 가능한 미래 시나리오를 플레이해라. 이 와중에 다른 이들이 너의 액션에 어떻게 반응할지 생각하고 계속해서 안알려진 곳을 탐색해라.
2. 만약에 이숙하지 않은 위치에 도달한다면 그 위치가 얼마나 가치 있는지 판단하고 이전 위치로 값을 전달해줘라.
3. 미래에 대한 가능성에 대한 생각이 끝난 후 너가 가장 많이 가본 행동을 취해라.
4. 게임이 마무리되면 돌아가서 너가 미래 위치의 값을 잘못 판단한 지점을 찾고 이해한 값으로 변경하라. 

## 그럼 알파제로를 어떻게 짤까
일단 알파고 제로의 CHEAT SHEET이다. 이걸 참고하라는데 일단은 좀만 참고만 하자. 

![사진](https://raw.githubusercontent.com/zoomKoding/zoomKoding.github.io/source/assets/_posts/connect4-2.png)

[알파고제로 설명이 잘되있는 사이트](http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/)

음... 아직은 이해가 안된다. 그럼 일단 나는 예제 코드로 바로 이동해서 접근해봐야겠다. 지금 블로그 작성한 사람이 사목에 알파고 제로를 적용한 코드를 이해해보자.

## 사목이란?
먼저 사목이 뭔지 이해해보자.

- 사목은 육목이나 다른 바둑판을 이용하는 게임과 달리 중력(?)이 기반이 된 게임이다.
- 육목이나 오목은 놓고 싶은 곳에 아무데나 놓을 수 있지만 사목은 돌들이 차곡차곡 쌓인다는 특징을 가지고 있다.
- 같은 돌이 어느 방향으로든 4번 반복되면 승리하게 된다.

![사진](https://raw.githubusercontent.com/zoomKoding/zoomKoding.github.io/source/assets/_posts/connect4-1.jpeg)

나름 사목을 이해하기 위해 앱도 깔아서 게임도 해봤다...ㅋㅋㅋㅋㅋ
(심심해서 한것도 없지 않아 있다 ^^;;)

## 코드
일단 클론해서 가져와 보자. 

### game.py

이 파일은 4목에 대한 게임의 룰이 설명되어 있다.
각 위치는 0부터 41까지 숫자로 명시되어 있다.

![사진](https://raw.githubusercontent.com/zoomKoding/zoomKoding.github.io/source/assets/_posts/connect4-3.png)

game.py에서는 하나의 게임 state에서 다른 state로 움직이는 로직을 제공한다.

예를 들어, 빈 판에다가 38에 돌을 놓는 액션이 있다면, **takeAction** method가 맨 밑에 줄 가운데 돌이 놔진 새로운 game state를 return 한다.

이 game.py 파일을 다른 게임 파일로 항상 대체 가능하다. 

### run.ipynb
이 파일은 학습시키는 코드를 포함하고 있다. 일단 처음에는 게임 규칙을 로드하고 알고리즘의 메인 루프를 돌게 한다. 메인 루프는 아래 3개의 과정을 포함하고 있다.

1. Self-play
2. Retraining the Neural Network
3. Evaluating the Neural Network

이 과정에서는 agent 두개가 존재한다. 하나는 **best_player**이고 다른 하나는 **current_player**이다.

best_player는 제일 잘 작동하는 neural network를 가지고 있고 self play memories를 만들어 주는데 사용한다. current_player는 자신의 neural network를 best_player가 만든 memories에 훈련시킨다. 그러다가 current_player가 승리하게 되면 current가 가진 neural network가 best_player 안에 있는 nueral network로 대체되게 되고 루프는 다시 시작하게 된다.

### agent.py
이 파일은 Agent(게임의 플레이어) 클래스를 담고 있다. 각 플레이어는 neural network와 MCTS로 초기화 되어있다.

**simulate** 함수는 몬테카를로 트리 서치 과정을 진행한다. 특히 agent는 tree의 leaf node로 이동해서 가지고 있는 neural network로 평가하고 그 값을 backfill해서 tree의 값을 업데이트 해준다.

**act** 함수는 simulation을 여러번 진행하면서 현재 위치에서 가장 favorable한 무브를 찾는다. 이 함수는 선택된 action을 return 해준다.

**replay** 함수는 이전 게임의 기억을 바탕으로 neural network를 학습시켜준다.  

### model.py
이건 일단 보류하자...

### MCTS.py
이 파일은 Node, Edge, MCTS class가 들어있다. MCTS classs는 **moveToLeaf**와 **backFill**이라는 함수와 각 Move의 수치를 담고 있는 Edge class 객체를 가지고 있다.

### config.py
key parameter를 가지고 있는 곳이다. 


## 깊은 분석

일단 내가 모델은 건드리기 쉽지 않을 것 같아서 game.py, funcs.py, agent.py 이렇게 세개 파일을 집중적으로 분석해서 주석 처리 해놨다. 

[game.py](https://github.com/zoomKoding/DeepReinforcementLearning/blob/master/games/connect6/game-connect4.py/)

[funcs.py](https://github.com/zoomKoding/DeepReinforcementLearning/blob/master/games/connect6/funcs-connect4.py/)


[agent.py](https://github.com/zoomKoding/DeepReinforcementLearning/blob/master/games/connect6/agent-connect4.py/)


## 함수를 정리하며

정확한 함수의 용도와 내부 구현은 실제 코드를 확인해봐야 할 것 같다.
내가 적용하려는 육목을 위해서는 일단 game.py를 수정해야하는데 아직 파이썬이 많이 어색한 내가 이거를 잘 처리할 수 있을지 솔직히 조금은 걱정이 앞선다. 
일단 내가 할 수 있는 한으로 짜보자!

